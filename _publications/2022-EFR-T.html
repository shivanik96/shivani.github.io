---
layout: publication
authors:
  - <b>Shivani Kumar</b>
  - Anubhav Shrimal
  - Md Shad Akhtar
  - Tanmoy Chakraborty
<!-- awards:
  - Invited to SIGGRAPH 2016 -->
highlight: true
link: https://github.com/LCS2-IIITD/Emotion-Flip-Reasoning.git
pdf: https://www.sciencedirect.com/science/article/pii/S0950705121011709
<!-- short_doi: 10/bdsz -->
tags:
  - Discovering emotion and reasoning its flip in multi-party conversations using masked memory network and transformer
  - Anubhav Shrimal
  - Md Shad Akhtar
  - Tanmoy Chakraborty
  - Knowledge-Based Systems
  - KBS
  - Journal
  - 2022
title: "Discovering emotion and reasoning its flip in multi-party conversations using masked memory network and transformer"
<!-- tweet: Visualization recommendation promotes breadth and prevents early fixation. -->
type:
  - Journal
venue: Knowledge-Based Systems
<!-- venue_location: Chicago, IL, USA -->
venue_tags:
  - KBS
<!-- venue_url: http://ieeevis.org/ -->
<!-- video: https://vimeo.com/135417594 -->
year: 2022
---

Efficient discovery of a speaker’s emotional states in a multi-party conversation is significant to design human-like conversational agents. During a conversation, the cognitive state of a speaker often alters due to certain past utterances, which may lead to a flip in their emotional state. Therefore, discovering the reasons (triggers) behind the speaker’s emotion-flip during a conversation is essential to explain the emotion labels of individual utterances. In this paper, along with addressing the task of emotion recognition in conversations (ERC), we introduce a novel task – Emotion-Flip Reasoning (EFR), that aims to identify past utterances which have triggered one’s emotional state to flip at a certain time. We propose a masked memory network to address the former and a Transformer-based network for the latter task. To this end, we consider MELD, a benchmark emotion recognition dataset in multi-party conversations for the task of ERC, and augment it with new ground-truth labels for EFR. An extensive comparison with five state-of-the-art models suggests improved performances of our models for both the tasks. We further present anecdotal evidence and both qualitative and quantitative error analyses to support the superiority of our models compared to the baselines.
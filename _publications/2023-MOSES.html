---
layout: publication
authors:
  - <b>Shivani Kumar</b>
  - Ishani Mondal
  - Md Shad Akhtar
  - Tanmoy Chakraborty
<!-- awards:
  - Invited to SIGGRAPH 2016 -->
highlight: true
link: https://github.com/LCS2-IIITD/MOSES.git
pdf: https://ojs.aaai.org/index.php/AAAI/article/view/26526
<!-- short_doi: 10/bdsz -->
tags:
  - Explaining (Sarcastic) Utterances to Enhance Affect Understanding in Multimodal Dialogues
  - Ishani Mondal
  - Md Shad Akhtar
  - Tanmoy Chakraborty
  - Association for the Advancement of Artificial Intelligence
  - AAAI
  - Conference
  - 2023
title: "Explaining (Sarcastic) Utterances to Enhance Affect Understanding in Multimodal Dialogues"
<!-- tweet: Visualization recommendation promotes breadth and prevents early fixation. -->
type:
  - Conference
venue: AAAI
venue_location: Washington DC, USA
venue_tags:
  - AAAI
<!-- venue_url: http://ieeevis.org/ -->
<!-- video: https://vimeo.com/135417594 -->
year: 2023
date: 2023-01-01
---

Conversations emerge as the primary media for exchanging ideas and conceptions. From the listener’s perspective, identifying various affective qualities, such as sarcasm, humour, and emotions, is paramount for comprehending the true connotation of the emitted utterance. However, one of the major hurdles faced in learning these affect dimensions is the presence of figurative language, viz. irony, metaphor, or sarcasm. We hypothesize that any detection system constituting the exhaustive and explicit presentation of the emitted utterance would improve the overall comprehension of the dialogue. To this end, we explore the task of Sarcasm Explanation in Dialogues, which aims to unfold the hidden irony behind sarcastic utterances. We propose MOSES, a deep neural network which takes a multimodal (sarcastic) dialogue instance as an input and generates a natural language sentence as its explanation. Subsequently, we leverage the generated explanation for various natural language understanding tasks in a conversational dialogue setup, such as sarcasm detection, humour identification, and emotion recognition. Our evaluation shows that MOSES outperforms the state-of-the-art system for SED by an average of ∼2% on different evaluation metrics, such as ROUGE, BLEU, and METEOR. Further, we observe that leveraging the generated explanation advances three downstream tasks for affect classification – an average improvement of ~14% F1-score in the sarcasm detection task and ∼2% in the humour identification and emotion recognition task. We also perform extensive analyses to assess the quality of the results.
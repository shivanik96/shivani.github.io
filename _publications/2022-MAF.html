---
layout: publication
authors:
  - <b>Shivani Kumar</b>
  - Atharva Kulkarni
  - Md Shad Akhtar
  - Tanmoy Chakraborty
<!-- awards:
  - Invited to SIGGRAPH 2016 -->
highlight: true
link: https://github.com/LCS2-IIITD/MAF.git
pdf: https://aclanthology.org/2022.acl-long.411/
<!-- short_doi: 10/bdsz -->
tags:
  - When did you become so smart, oh wise one?! Sarcasm Explanation in Multi-modal Multi-party Dialogues
  - Atharva Kulkarni
  - Md Shad Akhtar
  - Tanmoy Chakraborty
  - Association for Computational Linguistics 
  - ACL
  - Conference
  - 2022
title: "When did you become so smart, oh wise one?! Sarcasm Explanation in Multi-modal Multi-party Dialogues"
<!-- tweet: Visualization recommendation promotes breadth and prevents early fixation. -->
type:
  - Conference
venue: ACL
venue_location: Dublin, Ireland
venue_tags:
  - ACL
<!-- venue_url: http://ieeevis.org/ -->
<!-- video: https://vimeo.com/135417594 -->
year: 2022
---

Indirect speech such as sarcasm achieves a constellation of discourse goals in human communication. While the indirectness of figurative language warrants speakers to achieve certain pragmatic goals, it is challenging for AI agents to comprehend such idiosyncrasies of human communication. Though sarcasm identification has been a well-explored topic in dialogue analysis, for conversational systems to truly grasp a conversation{'}s innate meaning and generate appropriate responses, simply detecting sarcasm is not enough; it is vital to explain its underlying sarcastic connotation to capture its true essence. In this work, we study the discourse structure of sarcastic conversations and propose a novel task {--} Sarcasm Explanation in Dialogue (SED). Set in a multimodal and code-mixed setting, the task aims to generate natural language explanations of satirical conversations. To this end, we curate WITS, a new dataset to support our task. We propose MAF (Modality Aware Fusion), a multimodal context-aware attention and global information fusion module to capture multimodality and use it to benchmark WITS. The proposed attention module surpasses the traditional multimodal fusion baselines and reports the best performance on almost all metrics. Lastly, we carry out detailed analysis both quantitatively and qualitatively.